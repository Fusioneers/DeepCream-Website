{"site":{"data":{"attributes":{"pages":{"data":[{"attributes":{"title":"Home","content":[{"__typename":"ComponentPageLanding","header":null,"media":{"data":{"attributes":{"mime":"image/jpeg","url":"/uploads/scott_webb_FEQE_Qr_F5_M10_unsplash_small_a5a874f7c0.jpg","alternativeText":"Paul"}}}},{"__typename":"ComponentPageParagraph","title":"What is DeepCream?","body":"When gazing into the sky, there's one thing, or rather several things, that are very hard to miss - **the clouds/*. They come in all imaginable shapes and sizes and when looking at them most people cannot help but **interpret the clouds as all sorts of objects/*, be it cars, ships, animals or even people. One thing that might not come to one's mind however is the question of how that would look from space, but that is exactly the question we want to set out to answer in our Astro Pi project. The one obvious caveat is that we cannot go for ourselves, so we **want to train an AI to do the job for us./* But with that one in a lifetime chance to get footage directly from the ISS, it would be a shame to limit oneself to just recognizing funny shapes in the clouds.\n\nThat is where we thought about what other great things an AI might be able to read from the clouds, and we decided that it should be possible to also **gather information about the clouds, like the type, height and the wind speed./* And with that our project was born - the **Detection Estimation and Evaluation Program for Cloud Recognition and Electronic Analytic Monitoring - in short, DeepCream./*","important":true},{"__typename":"ComponentPageParagraph","title":"What are our Goals?","body":"DeepCream has two different goals, which consist of the scientific detection and interpretation and of the artistic classification of the clouds.\n\nFor the scientific part we **locate the clouds in a specific image/* and based on that, **determine the type and measure or calculate direction, speed, size as well as the height of the cloud./* With this data we might even be able to draw conclusions on what type of climate is predominant in those regions.\n\nThe artistic part on the other hand is based around the previously detected clouds will perform the **interpretation as a day-to-day object like a human would do./* This will hopefully yield interesting results about how clouds are interpreted differently from space, since they can usually only be seen from the ground.","important":null},{"__typename":"ComponentPageParagraph","title":"How will we achieve this?","body":"Our program consists of multiple parts, each contributing to the final result. \nFirstly the program **filters the clouds from the image./* We plan on achieving this by using the OpenCV library in python. In detail, we cut out the parts of the image whose HSV-values lie in a specific threshold. This threshold is dynamically adjusted by a smart algorithm to best fit the conditions under which the image was taken. Further, this smart algorithm determines when it’s nighttime and communicate this to the next part of our system.\n\nThat part **coordinates the taking of pictures and the evaluation of the footage./* The plan is to take pictures while orbiting on the sunlight side of the earth, and to run the artificial intelligences and other algorithms during the night. We can achieve this by keeping track of the images in a queue and multithreading those processes.\n\nThe different AI’s and algorithms can be grouped into scientific and artistic applications. For the scientific part of our project, the program analyses a given cloud through a series of filters to **determine the structure/* and from that conclude the wind direction and speed. Moreover, we will calculate the area of the cloud and with a classification AI determine its type. This will then allow us to **estimate roughly how high the clouds are./*\n\nLastly, another image classification AI will **interpret the cloud as a daily object like a car, tree or dog./*\n\nThe results will be saved in text form to a CSV file to save on storage. Additionally, we will compress the pictures taken and save as many as the remaining storage allows. ","important":null}]}},{"attributes":{"title":"DeepTeam","content":[{"__typename":"ComponentPageTeam","title":"Our team","team_members":{"data":[{"attributes":{"profile":{"data":{"attributes":{"mime":"image/jpeg","url":"/uploads/2021_04_pp_JK_544c42f363.jpeg","alternativeText":"Kevin"}}},"name":"Kevin Kretz","email":"mail@kevinkretz.de","github_username":"theKevinKretz","characterization":"Project lead, Pareidolia","link":"https://www.thekevinkretz.com/"}},{"attributes":{"profile":{"data":{"attributes":{"mime":"image/png","url":"/uploads/Profile_Picture_b09be88785.png","alternativeText":"Daniel"}}},"name":"Daniel Meiborg","email":"mail@danielmeiborg.com","github_username":"DanielMeiborg","characterization":"Image Analysis, Image Classification, Database and Logging","link":"http://www.danielmeiborg.com"}},{"attributes":{"profile":{"data":{"attributes":{"mime":"image/jpeg","url":"/uploads/scott_webb_FEQE_Qr_F5_M10_unsplash_small_a5a874f7c0.jpg","alternativeText":"Paul"}}},"name":"Paul Maier","email":"pauljustus279@gmail.com","github_username":"C0mput3r5c13nt15t","characterization":"Image Detection, Testing and Debugging, Website","link":"https://c0mput3r5c13nt15t.github.io/C0mput3r5c13nt15t/"}},{"attributes":{"profile":{"data":{"attributes":{"mime":"image/png","url":"/uploads/placeholder_2ba230f9f6.png","alternativeText":"placeholder.png"}}},"name":"Xinyue (Lucia) Guo","email":"xinyue.guo@s.birklehof.de","github_username":"aiculguo","characterization":"Research","link":null}},{"attributes":{"profile":{"data":{"attributes":{"mime":"image/png","url":"/uploads/placeholder_2ba230f9f6.png","alternativeText":"placeholder.png"}}},"name":"Jannis Sauer","email":"jannis.sauer@s.birklehof.de","github_username":null,"characterization":"Research","link":null}},{"attributes":{"profile":{"data":{"attributes":{"mime":"image/png","url":"/uploads/Microsoft_Teams_image_5f94e60403.png","alternativeText":"Lukas"}}},"name":"Lukas Pottgiesser","email":"lukas.pottgiesser@s.birklehof.de","github_username":null,"characterization":"Design Specialist","link":null}}]}}]}},{"attributes":{"title":"News","content":[{"__typename":"ComponentPageBlog","title":"News","blog_entries":{"data":[{"attributes":{"title":"Professional testing and submission","publishedAt":"2022-02-24T22:55:48.405Z","body":"Since the arrival of our Astro Pi kit we've been working hard on coding all the planned functionality but what is equally as important as writing code? Testing it, of course! With that in mind behold of DeepScreen - DeepTeams testing facility for DeepCream. The facility consists of the Astro Pi connected to the coral TPU and the camera with is held in position by a small lego tower. Facing it is a screen, which contrary to it's usual use, serves as a pinboard for the testing images.\n\nWith this testing facility we were able to simulate the conditions unter which the experiment will be run. Thanks to it we were able to find and patch a variety of bug and small inconsistencies before then finally submitting on the 24th Febrary 2022 at 22:30 - one and a half hours before the deadline.\n\nWe had a lot of fun up until now and are excited for the upcoming Phase 3.","media":{"data":[{"attributes":{"mime":"image/png","url":"/uploads/Microsoft_Teams_image_1_20b2f84f93.png","alternativeText":"DeepScreen"}}]}}},{"attributes":{"title":"Passed Phase 1 and received Astro Pi kit","publishedAt":"2021-12-31T15:34:37.286Z","body":"A few weeks ater having sent our mission proposal to Astro Pi we received an email telling us that we we're chosen for Phase 2 and that our AstroPi-Kit was soon going to arrive.\n\nAnd after the Christmas holydays it finally arrived. Since we are doing the 'Life on earth' expermient we will use the camera and for our AIs we want to use the Coral USB TPU. You can see the kit all set up and plugged in in the photo below.","media":{"data":[{"attributes":{"mime":"image/jpeg","url":"/uploads/astro_pi_kit_b045c2903a.jpg","alternativeText":"astro pi kit.jpg"}}]}}}]}}]}}]}}}}}